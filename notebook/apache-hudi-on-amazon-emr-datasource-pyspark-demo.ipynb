{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert, Update and Delete data on S3 Datalake using Apache Hudi and Amazon EMR - Demo\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Copy On Write](#Copy-On-Write)<br>\n",
    "    2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    2.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    2.3 [Deleting Records](#Deleting-Records)<br>\n",
    "3. [Rollback](#Rollback)<br>\n",
    "4. [Time travel with Hudi](#Time-travel-with-Hudi) \n",
    "5. [Merge on Read](#Merge-On-Read)<br>\n",
    "    5.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)<br>\n",
    "    5.2 [Batch Upsert some records](#Batch-Upsert-some-records)<br>\n",
    "    5.3 [Compaction for MOR tables](#Compaction-for-MOR-tables) <br>\n",
    "6. [Working with Partitioned Tables](#Working-with-Partitioned-Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write and Merge-On-Read tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Write a MultiKey Partitioned table as well as a Non-Partitioned table.\n",
    "- Tune the Bulk Insert write performance as per expected number of target files.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi table.\n",
    "- Delete from records from a Hudi table.\n",
    "- Understand how Hudi Commit Retention policy works.\n",
    "- Time travel with Hudi incremental tables using incremental and point-in-time queries\n",
    "- Perform Insert/Upsert/Delete operations for Merge-on-Read table and understand the difference between MOR and COW tables\n",
    "\n",
    "\n",
    "#### This demo runs fine on a 3 node (r4.4xlarge) EMR release 6.4 cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please replace ACCOUNTID with your current AWS account. Can be found in upper right section of your AWS Management Console. Your S3 bucket name should look like \"emr-workshop-197690357257-template\". 197690357257 is a sample account.\n",
    "\n",
    "Let's start by initializing the Spark Session to connect this notebook to our Spark EMR cluster:\n",
    "\n",
    "Note that the files hudi-spark-bundle.jar and spark-avro.jar are copied into HDFS.\n",
    "\n",
    "sudo su hadoop\n",
    "\n",
    "cd /home/hadoop\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar hdfs:///user/hadoop/\n",
    "\n",
    "hadoop fs -copyFromLocal /usr/lib/spark/jars/httpcore-4.4.11.jar hdfs:///user/hadoop/\n",
    "\n",
    "curl -o schema.avsc https://raw.githubusercontent.com/vasveena/hudi-workshop/main/LAB1/schema.avsc\n",
    "\n",
    "aws s3 cp schema.avsc s3://emr-workshop-ACCOUNTID-template/demos/schema/schema.avsc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer'}, 'proxyUser': 'assumed-role_TeamRole_MasterKey', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\", \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Following are the configuration items we will use in this demo\n",
    "\n",
    "Make sure to point the target parameter to your S3 bucket (replace your account ID in the S3 location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25103f834a3b453180c63094d81c3180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1637560469042_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-73.ec2.internal:20888/proxy/application_1637560469042_0008/\" class=\"emr-proxy-link\" emr-resource=\"j-1HCZO7EIKLFQY\n",
       "\" application-id=\"application_1637560469042_0008\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-184.ec2.internal:8042/node/containerlogs/container_1637560469042_0008_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_trips_table\",\n",
    "    \"target\": \"s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c827a02e03c4400895e28ec2b9a61d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.query.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825ec5fab41642f3a5a952bb8e8427f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40520951f103444f8fa17221f7aa47c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2021-11-22 07:56:59|\n",
      "|     New York|       B|      1|2021-11-22 07:56:59|\n",
      "|   New Jersey|       C|      2|2021-11-22 07:56:59|\n",
      "|  Los Angeles|       D|      3|2021-11-22 07:56:59|\n",
      "|    Las Vegas|       E|      4|2021-11-22 07:56:59|\n",
      "|       Tucson|       F|      5|2021-11-22 07:56:59|\n",
      "|Washington DC|       G|      6|2021-11-22 07:56:59|\n",
      "| Philadelphia|       H|      7|2021-11-22 07:56:59|\n",
      "|        Miami|       I|      8|2021-11-22 07:56:59|\n",
      "|San Francisco|       J|      9|2021-11-22 07:56:59|\n",
      "|      Seattle|       A|     10|2021-11-22 07:56:59|\n",
      "|     New York|       B|     11|2021-11-22 07:56:59|\n",
      "|   New Jersey|       C|     12|2021-11-22 07:56:59|\n",
      "|  Los Angeles|       D|     13|2021-11-22 07:56:59|\n",
      "|    Las Vegas|       E|     14|2021-11-22 07:56:59|\n",
      "|       Tucson|       F|     15|2021-11-22 07:56:59|\n",
      "|Washington DC|       G|     16|2021-11-22 07:56:59|\n",
      "| Philadelphia|       H|     17|2021-11-22 07:56:59|\n",
      "|        Miami|       I|     18|2021-11-22 07:56:59|\n",
      "|San Francisco|       J|     19|2021-11-22 07:56:59|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, dest))\n",
    "print(df1.count())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the data to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc6be8245e741fa8ef42c98f5c36792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected number of files is 3 files as BULK_INSERT_PARALLELISM is set to 3. \n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2021-10-10 23:51:57    0 Bytes .hoodie_$folder$\n",
    "2021-10-10 23:53:40   93 Bytes .hoodie_partition_metadata\n",
    "2021-10-10 23:53:50    4.8 MiB 128bbd02-25e8-4aa1-a651-3d1b20227f77-0_1-8-100_20211010235153.parquet\n",
    "2021-10-10 23:53:48    4.4 MiB 59043700-7191-4248-bb28-44f4f314e9d0-0_0-8-99_20211010235153.parquet\n",
    "2021-10-10 23:53:49    4.6 MiB fff0690b-28ca-4a63-8a08-b7d97020a802-0_2-8-101_20211010235153.parquet\n",
    "\n",
    "Total Objects: 5\n",
    "   Total Size: 13.8 MiB\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Let's inspect the table created and query the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7366605cc111465c97edba777309e597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+-----------+\n",
      "|database|tableName              |isTemporary|\n",
      "+--------+-----------------------+-----------+\n",
      "|default |hudi_mor_trips_table_ro|false      |\n",
      "|default |hudi_mor_trips_table_rt|false      |\n",
      "|default |hudi_trips_table       |false      |\n",
      "+--------+-----------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Glue data catalog for a new Hudi table called hudi_trips_table. Check the schema and notice the new columns added by Hudi to keep track of commits and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae10c7f376944ccb47c27b2cc236144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                    |destination  |route_id|trip_id|tstamp             |\n",
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "|20211122075711     |20211122075711_0_1  |0                 |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |0      |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_3  |1                 |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|New York     |B       |1      |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_5  |10                |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |10     |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_6  |100               |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |100    |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_9  |1000              |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |1000   |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_11 |10000             |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |10000  |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_14 |100000            |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |100000 |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_16 |1000000           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |1000000|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_18 |1000001           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|New York     |B       |1000001|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_20 |1000002           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|New Jersey   |C       |1000002|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_22 |1000003           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Los Angeles  |D       |1000003|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_24 |1000004           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Las Vegas    |E       |1000004|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_26 |1000005           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Tucson       |F       |1000005|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_28 |1000006           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Washington DC|G       |1000006|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_30 |1000007           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Philadelphia |H       |1000007|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_32 |1000008           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Miami        |I       |1000008|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_34 |1000009           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|San Francisco|J       |1000009|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_36 |100001            |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|New York     |B       |100001 |2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_38 |1000010           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|Seattle      |A       |1000010|2021-11-22 07:56:59|\n",
      "|20211122075711     |20211122075711_0_40 |1000011           |                      |7f5d6e0c-cd0e-45ab-9d80-308fe5beae28-0_0-8-131_20211122075711.parquet|New York     |B       |1000011|2021-11-22 07:56:59|\n",
      "+-------------------+--------------------+------------------+----------------------+---------------------------------------------------------------------+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df2.count()\n",
    "df2.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the Hive table as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ca4322c38740cc8d6dc52fe4501778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Upsert some records\n",
    "\n",
    "Let's modify a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1707a79ed9498782e85ea0ae4ad6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2021-11-22 07:56:59|\n",
      "|1000001|B       |New York     |2021-11-22 07:56:59|\n",
      "|1000002|C       |New Jersey   |2021-11-22 07:56:59|\n",
      "|1000003|D       |Los Angeles  |2021-11-22 07:56:59|\n",
      "|1000004|E       |Las Vegas    |2021-11-22 07:56:59|\n",
      "|1000005|F       |Tucson       |2021-11-22 07:56:59|\n",
      "|1000006|G       |Washington DC|2021-11-22 07:56:59|\n",
      "|1000007|H       |Philadelphia |2021-11-22 07:56:59|\n",
      "|1000008|I       |Miami        |2021-11-22 07:56:59|\n",
      "|1000009|J       |San Francisco|2021-11-22 07:56:59|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\" where trip_id between 1000000 and 1000009\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a6a1d1ba4c47689cccec74badd23e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"Boston\", \"Boston\", \"Boston\", \"Boston\", \"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\",\"Boston\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41622366476145afa13b8cffa9934865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-----------+\n",
      "|trip_id|route_id|             tstamp|destination|\n",
      "+-------+--------+-------------------+-----------+\n",
      "|1000000|       A|2021-11-22 08:00:13|     Boston|\n",
      "|1000001|       B|2021-11-22 08:00:13|     Boston|\n",
      "|1000002|       C|2021-11-22 08:00:13|     Boston|\n",
      "|1000003|       D|2021-11-22 08:00:13|     Boston|\n",
      "|1000004|       E|2021-11-22 08:00:13|     Boston|\n",
      "|1000005|       F|2021-11-22 08:00:13|     Boston|\n",
      "|1000006|       G|2021-11-22 08:00:13|     Boston|\n",
      "|1000007|       H|2021-11-22 08:00:13|     Boston|\n",
      "|1000008|       I|2021-11-22 08:00:13|     Boston|\n",
      "|1000009|       J|2021-11-22 08:00:13|     Boston|\n",
      "+-------+--------+-------------------+-----------+"
     ]
    }
   ],
   "source": [
    "df3.select(\"trip_id\",\"route_id\",\"tstamp\",\"destination\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the destination and timestamp for trip IDs 1000000 to 1000010. Now, let's upsert the changes to S3. Note that the operation now is \"Upsert\" as opposed to BulkInsert for the initial load:\n",
    "\n",
    "```\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cb90bf5e544494bf5117a2344ac70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce38028046114f31af07930384b8248f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1000000|A       |2021-11-22 08:00:13|Boston       |\n",
      "|1000001|B       |2021-11-22 08:00:13|Boston       |\n",
      "|1000002|C       |2021-11-22 08:00:13|Boston       |\n",
      "|1000003|D       |2021-11-22 08:00:13|Boston       |\n",
      "|1000004|E       |2021-11-22 08:00:13|Boston       |\n",
      "|1000005|F       |2021-11-22 08:00:13|Boston       |\n",
      "|1000006|G       |2021-11-22 08:00:13|Boston       |\n",
      "|1000007|H       |2021-11-22 08:00:13|Boston       |\n",
      "|1000008|I       |2021-11-22 08:00:13|Boston       |\n",
      "|1000009|J       |2021-11-22 08:00:13|Boston       |\n",
      "|1000010|A       |2021-11-22 07:56:59|Seattle      |\n",
      "|1000011|B       |2021-11-22 07:56:59|New York     |\n",
      "|1000012|C       |2021-11-22 07:56:59|New Jersey   |\n",
      "|1000013|D       |2021-11-22 07:56:59|Los Angeles  |\n",
      "|999996 |G       |2021-11-22 07:56:59|Washington DC|\n",
      "|999997 |H       |2021-11-22 07:56:59|Philadelphia |\n",
      "|999998 |I       |2021-11-22 07:56:59|Miami        |\n",
      "|999999 |J       |2021-11-22 07:56:59|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id between 999996 and 1000013\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the commit\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2021-10-10 23:51:57    0 Bytes .hoodie_$folder$\n",
    "2021-10-10 23:53:40   93 Bytes .hoodie_partition_metadata\n",
    "2021-10-10 23:53:50    4.8 MiB 128bbd02-25e8-4aa1-a651-3d1b20227f77-0_1-8-100_20211010235153.parquet\n",
    "2021-10-10 23:58:45    4.4 MiB 59043700-7191-4248-bb28-44f4f314e9d0-0_0-55-538_20211010235834.parquet\n",
    "2021-10-10 23:53:48    4.4 MiB 59043700-7191-4248-bb28-44f4f314e9d0-0_0-8-99_20211010235153.parquet\n",
    "2021-10-10 23:53:49    4.6 MiB fff0690b-28ca-4a63-8a08-b7d97020a802-0_2-8-101_20211010235153.parquet\n",
    "\n",
    "Total Objects: 6\n",
    "   Total Size: 18.2 MiB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we upserted some records, let us try to insert 10 new records into the table. We will use same upsert option. As primary keys 2000000 to 2000009 do not exist in the table, the records will be inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c49748f441471f8a74c2e41bfe81a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2021-11-22 08:00:47|\n",
      "|   Syracuse|       B|2000001|2021-11-22 08:00:47|\n",
      "|   Syracuse|       C|2000002|2021-11-22 08:00:47|\n",
      "|   Syracuse|       D|2000003|2021-11-22 08:00:47|\n",
      "|   Syracuse|       E|2000004|2021-11-22 08:00:47|\n",
      "|   Syracuse|       F|2000005|2021-11-22 08:00:47|\n",
      "|   Syracuse|       G|2000006|2021-11-22 08:00:47|\n",
      "|   Syracuse|       H|2000007|2021-11-22 08:00:47|\n",
      "|   Syracuse|       I|2000008|2021-11-22 08:00:47|\n",
      "|   Syracuse|       J|2000009|2021-11-22 08:00:47|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "insert_dest = [\"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\", \"Syracuse\"]\n",
    "df5 = create_json_df(spark, get_json_data(2000000, 10, insert_dest))\n",
    "df5.count()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3e6d0b74b448a6811e51105f3b3a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f8203641744246b9e3d64282863b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000010"
     ]
    }
   ],
   "source": [
    "df6=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911e4b2156e84f58956ae87eb2fa3e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|2000009|J       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000008|I       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000007|H       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000006|G       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000001|B       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000000|A       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000005|F       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000004|E       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000003|D       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000002|C       |2021-11-22 08:00:47|Syracuse     |\n",
      "|1999997|H       |2021-11-22 07:56:59|Philadelphia |\n",
      "|1999998|I       |2021-11-22 07:56:59|Miami        |\n",
      "|1999999|J       |2021-11-22 07:56:59|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"Syracuse\" destination we added to the table. Note that the only change is the single line that set the hoodie.datasource.write.payload.class to org.apache.hudi.EmptyHoodieRecordPayload to delete the records.\n",
    "\n",
    "```\n",
    ".option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795c1610f97f49a8819f5efc5ae4bc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|   Syracuse|       A|2000000|2021-11-22 08:00:47|\n",
      "|   Syracuse|       B|2000001|2021-11-22 08:00:47|\n",
      "|   Syracuse|       C|2000002|2021-11-22 08:00:47|\n",
      "|   Syracuse|       D|2000003|2021-11-22 08:00:47|\n",
      "|   Syracuse|       E|2000004|2021-11-22 08:00:47|\n",
      "|   Syracuse|       F|2000005|2021-11-22 08:00:47|\n",
      "|   Syracuse|       G|2000006|2021-11-22 08:00:47|\n",
      "|   Syracuse|       H|2000007|2021-11-22 08:00:47|\n",
      "|   Syracuse|       I|2000008|2021-11-22 08:00:47|\n",
      "|   Syracuse|       J|2000009|2021-11-22 08:00:47|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d8a2f2c6124bafba4e88e5293bb6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e36a651e0c043708e252251c50dfb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|1999997|H       |2021-11-22 07:56:59|Philadelphia |\n",
      "|1999998|I       |2021-11-22 07:56:59|Miami        |\n",
      "|1999999|J       |2021-11-22 07:56:59|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records > 2000000 no longer exist in our table.\n",
    "\n",
    "Let's observe the number of files in S3. Expected : 6 files (initial files (3) + one upsert + one insert + one delete = 6)\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:15:43    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-51-609_20200428231528.parquet\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:18:24    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-135-1376_20200428231814.parquet\n",
    "2020-04-28 23:17:17    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-93-1007_20200428231705.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 8\n",
    "   Total Size: 27.5 MiB\n",
    "\n",
    "```\n",
    "\n",
    "In our example, we set number of commits to retain as 10. So, maximum only 10 new files can be created on top of our bulk insert files. i.e., 13 files in total. If we had set the commits_to_retain as 2, the number of files created will not increase beyond initial files(3) + commits_to_retain(2) = 5 files. This is because Hudi Cleaning Policy will delete older files when writing based on the commit retain policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback\n",
    "\n",
    "Let's say we want to roll back the last delete we made. \n",
    "\n",
    "```\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect --path s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table\n",
    "\n",
    "commits show\n",
    "\n",
    "hudi:hudi_trips_table->commits show\n",
    "2021-10-11 00:02:55,419 INFO timeline.HoodieActiveTimeline: Loaded instants [[20211010235153__commit__COMPLETED], [20211010235834__commit__COMPLETED], [20211010235951__commit__COMPLETED], [20211011000109__commit__COMPLETED]]\n",
    "2021-10-11 00:02:55,439 INFO s3n.S3NativeFileSystem: Opening 's3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table/.hoodie/20211011000109.commit' for reading\n",
    "2021-10-11 00:02:55,747 INFO s3n.S3NativeFileSystem: Opening 's3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table/.hoodie/20211010235951.commit' for reading\n",
    "2021-10-11 00:02:55,829 INFO s3n.S3NativeFileSystem: Opening 's3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table/.hoodie/20211010235834.commit' for reading\n",
    "2021-10-11 00:02:55,907 INFO s3n.S3NativeFileSystem: Opening 's3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table/.hoodie/20211010235153.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20211011000109 │ 4.4 MB              │ 0                 │ 1                   │ 1                        │ 635786                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20211010235951 │ 4.4 MB              │ 0                 │ 1                   │ 1                        │ 635796                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20211010235834 │ 4.4 MB              │ 0                 │ 1                   │ 1                        │ 635786                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20211010235153 │ 13.8 MB             │ 3                 │ 0                   │ 1                        │ 2000000               │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "commit rollback --commit 20211011000109\n",
    "\n",
    "....\n",
    "\n",
    "Commit 20211011000109 rolled back\n",
    "\n",
    "```\n",
    "    \n",
    "If job doesn't complete in 5 mins, terminate the hudi-cli.sh using Ctrl+C, open Hudi CLI again and issue \"commits show\". You should see 3 instead of 4 commits. Now let us check what happened to the records we deleted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61b8fd41e2043ea8efba6543d1cc6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+-------------+\n",
      "|trip_id|route_id|tstamp             |destination  |\n",
      "+-------+--------+-------------------+-------------+\n",
      "|2000009|J       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000008|I       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000007|H       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000006|G       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000001|B       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000000|A       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000005|F       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000004|E       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000003|D       |2021-11-22 08:00:47|Syracuse     |\n",
      "|2000002|C       |2021-11-22 08:00:47|Syracuse     |\n",
      "|1999997|H       |2021-11-22 07:56:59|Philadelphia |\n",
      "|1999998|I       |2021-11-22 07:56:59|Miami        |\n",
      "|1999999|J       |2021-11-22 07:56:59|San Francisco|\n",
      "+-------+--------+-------------------+-------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel with Hudi \n",
    "\n",
    "Now, let us time travel with Hudi (query previous commits) with point-in-time queries\n",
    "\n",
    "\n",
    "In EMR cluster, let us check Hudi CLI and check how commits look right now \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect --path s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_trips_table\n",
    "\n",
    "commits show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044543ba432b459e90d263d335702ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|commitTime    |\n",
      "+--------------+\n",
      "|20211122075711|\n",
      "|20211122080022|\n",
      "|20211122080050|\n",
      "+--------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_trips_table order by commitTime\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check out point-in-time queries. i.e., query from a specific commit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0318b007339c4e468bccba4ec76127de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: \n",
      "Row(commitTime='20211122075711')\n",
      "Row(commitTime='20211122080022')\n",
      "Row(commitTime='20211122080050')"
     ]
    }
   ],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "for elem in commits: print (elem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c3406b5c3a4d6d9ac6881adf81cfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 20211122075711"
     ]
    }
   ],
   "source": [
    "#Lets check out the first state. i.e., before we updated the trip to Boston\n",
    "\n",
    "beginTime = \"000\" #Represents all commits > this time.\n",
    "endTime = commits[-3][0] # first commit\n",
    "print(\"end time: \"+endTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .option(END_INSTANTTIME_OPT_KEY, endTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6c684db18e4de5abe3c49a83fa7c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-------------+-------------------+\n",
      "|_hoodie_commit_time|trip_id|route_id|  destination|             tstamp|\n",
      "+-------------------+-------+--------+-------------+-------------------+\n",
      "|     20211122075711|1000000|       A|      Seattle|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000001|       B|     New York|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000002|       C|   New Jersey|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000003|       D|  Los Angeles|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000004|       E|    Las Vegas|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000005|       F|       Tucson|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000006|       G|Washington DC|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000007|       H| Philadelphia|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000008|       I|        Miami|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000009|       J|San Francisco|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000010|       A|      Seattle|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000011|       B|     New York|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000012|       C|   New Jersey|2021-11-22 07:56:59|\n",
      "|     20211122075711|1000013|       D|  Los Angeles|2021-11-22 07:56:59|\n",
      "|     20211122075711| 999996|       G|Washington DC|2021-11-22 07:56:59|\n",
      "|     20211122075711| 999997|       H| Philadelphia|2021-11-22 07:56:59|\n",
      "|     20211122075711| 999998|       I|        Miami|2021-11-22 07:56:59|\n",
      "|     20211122075711| 999999|       J|San Francisco|2021-11-22 07:56:59|\n",
      "+-------------------+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the records from 1000000 to 1000009 display first state of our table before we upserted the trip destination to Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge On Read \n",
    "\n",
    "The default table type is Copy-On-Write which is best suited for read-heavy workloads with modest writes. Copy-On-Write creates commit files with original data + the new changes during writing itself. While this increases latency on writes, this set up makes it more manageable for faster read.\n",
    "\n",
    "For near real-time applications that mandate quick upserts, MERGE_ON_READ table type would be better suited. MOR table stores incoming upserts for each file group, onto a row based delta log (In Avro file format). This log is then merged with the existing Parquet file using a a compactor during reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a921abcf7840e1a8831ec8c6fd20fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_mor_trips_table\",\n",
    "    \"table_name_rt\": \"hudi_mor_trips_table_rt\",\n",
    "    \"target\": \"s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_mor_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843a1e44994f470ab0c5e64084ff2ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STORAGE_TYPE_OPT_KEY=\"hoodie.datasource.write.storage.type\"\n",
    "COMPACTION_INLINE_OPT_KEY=\"hoodie.compact.inline\"\n",
    "COMPACTION_MAX_DELTA_COMMITS_OPT_KEY=\"hoodie.compact.inline.max.delta.commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b1e679fb1f4e6184607dff3a4fd3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-------+-------------------+\n",
      "|  destination|route_id|trip_id|             tstamp|\n",
      "+-------------+--------+-------+-------------------+\n",
      "|      Seattle|       A|      0|2021-11-22 08:05:17|\n",
      "|     New York|       B|      1|2021-11-22 08:05:17|\n",
      "|   New Jersey|       C|      2|2021-11-22 08:05:17|\n",
      "|  Los Angeles|       D|      3|2021-11-22 08:05:17|\n",
      "|    Las Vegas|       E|      4|2021-11-22 08:05:17|\n",
      "|       Tucson|       F|      5|2021-11-22 08:05:17|\n",
      "|Washington DC|       G|      6|2021-11-22 08:05:17|\n",
      "| Philadelphia|       H|      7|2021-11-22 08:05:17|\n",
      "|        Miami|       I|      8|2021-11-22 08:05:17|\n",
      "|San Francisco|       J|      9|2021-11-22 08:05:17|\n",
      "|      Seattle|       A|     10|2021-11-22 08:05:17|\n",
      "|     New York|       B|     11|2021-11-22 08:05:17|\n",
      "|   New Jersey|       C|     12|2021-11-22 08:05:17|\n",
      "|  Los Angeles|       D|     13|2021-11-22 08:05:17|\n",
      "|    Las Vegas|       E|     14|2021-11-22 08:05:17|\n",
      "|       Tucson|       F|     15|2021-11-22 08:05:17|\n",
      "|Washington DC|       G|     16|2021-11-22 08:05:17|\n",
      "| Philadelphia|       H|     17|2021-11-22 08:05:17|\n",
      "|        Miami|       I|     18|2021-11-22 08:05:17|\n",
      "|San Francisco|       J|     19|2021-11-22 08:05:17|\n",
      "+-------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "mor_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df2 = create_json_df(spark, get_json_data(0, 2000000, mor_dest))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk insert will take the same time as COW as this is the first write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b7f02cde3644fa9fe4893a0d31ce1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"1\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of files \n",
    "\n",
    "Let us check the contents of S3 path. Bulk insert operation on Copy-On-Write and Merge-On-Read tables is identical in terms of performance. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2021-10-11 00:11:27          0 .hoodie_$folder$\n",
    "2021-10-11 00:13:05         93 .hoodie_partition_metadata\n",
    "2021-10-11 00:13:13    4887456 341cc9bf-8cf7-40b6-be2d-8ca55d6c13b6-0_1-186-1671_20211011001125.parquet\n",
    "2021-10-11 00:13:14    4715858 790fd827-fa27-4748-8e05-c82f3ceeb399-0_0-186-1670_20211011001125.parquet\n",
    "2021-10-11 00:13:16    4897929 fbe029af-6254-4317-9718-01d37cf3ecdd-0_2-186-1672_20211011001125.parquet\n",
    "```\n",
    "\n",
    "Notice the delta commits \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_mor_trips_table/.hoodie/\n",
    "                           PRE .aux/\n",
    "2021-10-11 00:11:27          0 .aux_$folder$\n",
    "2021-10-11 00:11:27          0 .temp_$folder$\n",
    "2021-10-11 00:13:17       3193 20211011001125.deltacommit\n",
    "2021-10-11 00:12:16          0 20211011001125.deltacommit.inflight\n",
    "2021-10-11 00:11:29          0 20211011001125.deltacommit.requested\n",
    "2021-10-11 00:11:27          0 archived_$folder$\n",
    "2021-10-11 00:11:28        333 hoodie.properties\n",
    "```\n",
    "\n",
    "This is the first commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to upsert some records into MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18eddb6c6df74095884b669508e8fdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|  San Diego|       A|1000000|2021-11-22 08:07:00|\n",
      "|  San Diego|       B|1000001|2021-11-22 08:07:00|\n",
      "|  San Diego|       C|1000002|2021-11-22 08:07:00|\n",
      "|  San Diego|       D|1000003|2021-11-22 08:07:00|\n",
      "|  San Diego|       E|1000004|2021-11-22 08:07:00|\n",
      "|  San Diego|       F|1000005|2021-11-22 08:07:00|\n",
      "|  San Diego|       G|1000006|2021-11-22 08:07:00|\n",
      "|  San Diego|       H|1000007|2021-11-22 08:07:00|\n",
      "|  San Diego|       I|1000008|2021-11-22 08:07:00|\n",
      "|  San Diego|       J|1000009|2021-11-22 08:07:00|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca2c18b47494f71926fb02fd961eaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"1\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f918dad3574a1e855cdd843a2ad858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |Seattle      |2021-11-22 08:05:17|\n",
      "|1000001|B       |New York     |2021-11-22 08:05:17|\n",
      "|1000002|C       |New Jersey   |2021-11-22 08:05:17|\n",
      "|1000003|D       |Los Angeles  |2021-11-22 08:05:17|\n",
      "|1000004|E       |Las Vegas    |2021-11-22 08:05:17|\n",
      "|1000005|F       |Tucson       |2021-11-22 08:05:17|\n",
      "|1000006|G       |Washington DC|2021-11-22 08:05:17|\n",
      "|1000007|H       |Philadelphia |2021-11-22 08:05:17|\n",
      "|1000008|I       |Miami        |2021-11-22 08:05:17|\n",
      "|1000009|J       |San Francisco|2021-11-22 08:05:17|\n",
      "|1000010|A       |Seattle      |2021-11-22 08:05:17|\n",
      "|999996 |G       |Washington DC|2021-11-22 08:05:17|\n",
      "|999997 |H       |Philadelphia |2021-11-22 08:05:17|\n",
      "|999998 |I       |Miami        |2021-11-22 08:05:17|\n",
      "|999999 |J       |San Francisco|2021-11-22 08:05:17|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+ \"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the real-time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8441d8ab7441aaa83ea120dbece5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+-------------------+\n",
      "|trip_id|route_id|destination  |tstamp             |\n",
      "+-------+--------+-------------+-------------------+\n",
      "|1000000|A       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000001|B       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000002|C       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000003|D       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000004|E       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000005|F       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000006|G       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000007|H       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000008|I       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000009|J       |San Diego    |2021-11-22 08:07:00|\n",
      "|1000010|A       |Seattle      |2021-11-22 08:05:17|\n",
      "|999996 |G       |Washington DC|2021-11-22 08:05:17|\n",
      "|999997 |H       |Philadelphia |2021-11-22 08:05:17|\n",
      "|999998 |I       |Miami        |2021-11-22 08:05:17|\n",
      "|999999 |J       |San Francisco|2021-11-22 08:05:17|\n",
      "+-------+--------+-------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+ config['table_name_rt'] + \" where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the S3 path again. There is no change in number of Parquet files after upsert operation unlike Copy-On-Write tables. \n",
    "\n",
    "```\n",
    "$  aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2021-10-11 02:13:07       2071 .1dc7e87f-2160-4a6b-b3b4-3c89f6170261-0_20211011021111.log.1_0-197-1880\n",
    "2021-10-11 02:11:12          0 .hoodie_$folder$\n",
    "2021-10-11 02:12:41         93 .hoodie_partition_metadata\n",
    "2021-10-11 02:12:49    4761546 1dc7e87f-2160-4a6b-b3b4-3c89f6170261-0_0-163-1551_20211011021111.parquet\n",
    "2021-10-11 02:12:48    4648132 49e19858-1ae8-493b-9bbe-06d90ec74983-0_2-163-1553_20211011021111.parquet\n",
    "2021-10-11 02:12:50    5198196 578042a0-fe9d-4c2d-adfc-310a06b78494-0_1-163-1552_20211011021111.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Now, let us compact using Hudi CLI \n",
    "\n",
    "On EMR:\n",
    "\n",
    "```\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "hudi->connect --path s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "╔═════════════════════════╤═══════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╧═══════╧═══════════════════════════════╣\n",
    "║ (empty)                                                         ║\n",
    "╚═════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "```\n",
    "\n",
    "Notice there are no compactions for our mor table. Let us manually schedule and run a compaction. \n",
    "\n",
    "```\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction schedule --hoodieConfigs hoodie.datasource.hive_sync.partition_extractor_class=com.uber.hoodie.NonpartitionedKeyGenerator,hoodie.compact.inline.max.delta.commits=1\n",
    "\n",
    "Attempted to schedule compaction for 20211011021355\n",
    "\n",
    "Once you see the above message, reconnect to Hudi CLI and issue \"connect --path\" command again. \n",
    "\n",
    "hudi:hudi_mor_trips_table->connect --path s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "20/04/28 23:39:04 INFO timeline.HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@240d2f9d\n",
    "20/04/28 23:39:04 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/demos/hudi/hudi_mor_trips_table/.hoodie/.aux/20211011021355.compaction.requested' for reading\n",
    "╔═════════════════════════╤═══════════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State     │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╪═══════════╪═══════════════════════════════╣\n",
    "║ 20211011021355          │ REQUESTED │ 1                             ║\n",
    "╚═════════════════════════╧═══════════╧═══════════════════════════════╝\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction run --parallelism 10 --sparkMemory 4G --retry 2 --schemaFilePath s3://emr-workshop-ACCOUNTID-template/demos/schema/schema.avsc --compactionInstant 20211011021355\n",
    "\n",
    "Compaction successfully completed for 20211011021355\n",
    "\n",
    "```\n",
    "\n",
    "You can also schedule a compaction using \"compaction schedule\" option. \n",
    "\n",
    "Now, if we check the S3 path, we will see the delta commit has been merged into a new file \n",
    "\n",
    "```\n",
    "$  aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2021-10-11 02:13:07       2071 .1dc7e87f-2160-4a6b-b3b4-3c89f6170261-0_20211011021111.log.1_0-197-1880\n",
    "2021-10-11 02:11:12          0 .hoodie_$folder$\n",
    "2021-10-11 02:12:41         93 .hoodie_partition_metadata\n",
    "2021-10-11 02:17:42    4760914 1dc7e87f-2160-4a6b-b3b4-3c89f6170261-0_0-0-0_20211011021355.parquet\n",
    "2021-10-11 02:12:49    4761546 1dc7e87f-2160-4a6b-b3b4-3c89f6170261-0_0-163-1551_20211011021111.parquet\n",
    "2021-10-11 02:12:48    4648132 49e19858-1ae8-493b-9bbe-06d90ec74983-0_2-163-1553_20211011021111.parquet\n",
    "2021-10-11 02:12:50    5198196 578042a0-fe9d-4c2d-adfc-310a06b78494-0_1-163-1552_20211011021111.parquet\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Tables\n",
    "\n",
    "Let's do the same thing with Partitioned Tables. For the sake of this demo, we will be making route_id as partition field. You can also have a nested partition structure like yyyy/mm/dd which is more common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27523e50a9e24cbbaccc2cf8a0a07d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_partitioned_trips_table\",\n",
    "    \"target\": \"s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_partitioned_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "    \"partition_keys\" : \"route_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f057572538ed45c3bfd1b2dfb80c87e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "part_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, part_dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the partitionKey column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b457e0a8c214ea793c8d8aaee934a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  route_id|\n",
      "+----------+\n",
      "|route_id=A|\n",
      "|route_id=B|\n",
      "|route_id=C|\n",
      "|route_id=D|\n",
      "|route_id=E|\n",
      "+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "hudiTablePartitionKey=\"route_id\"\n",
    "df1 = df1.withColumn(hudiTablePartitionKey,concat(lit(\"route_id=\"),col(\"route_id\")))\n",
    "df1.select(hudiTablePartitionKey).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now write out the data to S3. Notice that the Hive Partition Extractor class has changed in the statement below:\n",
    "\n",
    "```\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52b7c791c59434ba430334b40dd91d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 6)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions fields are present in our Hive table from Glue catalog\n",
    "\n",
    "```\n",
    "PARTITIONED BY (`route_id` STRING)\n",
    "```\n",
    "\n",
    "Let's now query the data and group by the the partition columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70992f6c5e864efc9a6f93a5de5d16d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|route_id|num_trips|\n",
      "+--------+---------+\n",
      "|A       |200000   |\n",
      "|B       |200000   |\n",
      "|C       |200000   |\n",
      "|D       |200000   |\n",
      "|E       |200000   |\n",
      "|F       |200000   |\n",
      "|G       |200000   |\n",
      "|H       |200000   |\n",
      "|I       |200000   |\n",
      "|J       |200000   |\n",
      "+--------+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select route_id, count(*) as num_trips from \"+config['table_name']+\" group by route_id order by route_id\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the S3 path\n",
    "\n",
    "```\n",
    "$ $ aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_partitioned_trips_table/\n",
    "                           PRE .hoodie/\n",
    "                           PRE route_id=A/\n",
    "                           PRE route_id=B/\n",
    "                           PRE route_id=C/\n",
    "                           PRE route_id=D/\n",
    "                           PRE route_id=E/\n",
    "                           PRE route_id=F/\n",
    "                           PRE route_id=G/\n",
    "                           PRE route_id=H/\n",
    "                           PRE route_id=I/\n",
    "                           PRE route_id=J/\n",
    "2021-10-11 02:19:21          0 .hoodie_$folder$\n",
    "2021-10-11 02:21:07          0 route_id=A_$folder$\n",
    "2021-10-11 02:21:06          0 route_id=B_$folder$\n",
    "2021-10-11 02:21:08          0 route_id=C_$folder$\n",
    "2021-10-11 02:21:07          0 route_id=D_$folder$\n",
    "2021-10-11 02:21:10          0 route_id=E_$folder$\n",
    "2021-10-11 02:21:06          0 route_id=F_$folder$\n",
    "2021-10-11 02:21:06          0 route_id=G_$folder$\n",
    "2021-10-11 02:21:09          0 route_id=H_$folder$\n",
    "2021-10-11 02:21:07          0 route_id=I_$folder$\n",
    "2021-10-11 02:21:10          0 route_id=J_$folder$\n",
    "\n",
    "```\n",
    "\n",
    "Under each partition, there will be partition metadata\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://emr-workshop-ACCOUNTID-template/demos/hudi/hudi_partitioned_trips_table/route_id=A/\n",
    "2021-10-11 02:21:07         93 .hoodie_partition_metadata\n",
    "2021-10-11 02:21:10    1723511 0e3f96bc-f05e-46a9-912b-156dccad1559-0_0-222-2111_20211011021919.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other operations Upsert, Insert, Delete etc. behave the same way on Partitioned tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
