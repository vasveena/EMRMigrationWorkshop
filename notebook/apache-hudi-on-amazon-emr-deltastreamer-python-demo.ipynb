{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Apache Hudi Deltastreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Apache Hudi Deltastreamer\n",
    "HoodieDeltaStreamer utility is part of hudi-utilities-bundle that provides a way to ingest data from sources such as DFS or Kafka.\n",
    "\n",
    "In this notebook, you will learn to use DeltaStreamer Utility to bulk insert data into a Hudi Dataset as a Copy on Write(CoW) table and perform batch upsert. \n",
    "\n",
    "We will run queries in hudi-cli and SparkSQL to verify the tables and subsequent updates are incorporated into our datalake on Amazon S3\n",
    "\n",
    "Let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python Faker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Faker\n",
      "  Downloading Faker-9.2.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 23.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /opt/conda/lib/python3.7/site-packages (from Faker) (2.8.1)\n",
      "Collecting text-unidecode==1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.4->Faker) (1.14.0)\n",
      "Installing collected packages: text-unidecode, Faker\n",
      "Successfully installed Faker-9.2.0 text-unidecode-1.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Profile Generator\n",
    "\n",
    "Fake profile generator uses Python's Faker [https://faker.readthedocs.io/en/master/index.html] library. Let's define a method to generate a number of random person profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "import io\n",
    "from io import StringIO\n",
    "from faker import Faker\n",
    "from faker.providers import date_time, credit_card\n",
    "from json import dumps\n",
    "\n",
    "\n",
    "# Intialize Faker library and S3 client\n",
    "fake = Faker() \n",
    "fake.add_provider(date_time)\n",
    "fake.add_provider(credit_card)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Write the fake profile data to a S3 bucket\n",
    "# Replace with your own bucket\n",
    "s3_bucket = \"vasveena-test-demo\"\n",
    "s3_load_prefix = 'hudi-ds/inputdata/'\n",
    "s3_update_prefix = 'hudi-ds/updates/'\n",
    "\n",
    "# Number of records in each file and number of files\n",
    "# Adjust per your need - this produces 40MB files\n",
    "#num_records = 150000\n",
    "#num_files = 50\n",
    "\n",
    "num_records = 10000\n",
    "num_files = 15\n",
    "\n",
    "def generate_bulk_data():\n",
    "    '''\n",
    "    Generates bulk profile data\n",
    "    '''\n",
    "    # Generate number of files equivalent to num_files\n",
    "    for i in range (num_files):\n",
    "        fake_profile_data = fake_profile_generator(num_records, fake)\n",
    "        fakeIO = StringIO()\n",
    "        filename = 'profile_' + str(i + 1) + '.json'\n",
    "        s3key = s3_load_prefix + filename \n",
    "\n",
    "        fakeIO.write(str(''.join(dumps_lines(fake_profile_data))))\n",
    "\n",
    "        s3object = s3.Object(s3_bucket, s3key)\n",
    "        s3object.put(Body=(bytes(fakeIO.getvalue().encode('UTF-8'))))\n",
    "        fakeIO.close()\n",
    "\n",
    "def generate_updates():\n",
    "    '''\n",
    "    Generates updates for the profiles\n",
    "    '''\n",
    "    #\n",
    "    # We will make updates to records in randomly picked files\n",
    "    #\n",
    "    random_file_list = []\n",
    "    \n",
    "    for i in range (1, num_files):\n",
    "        random_file_list.append('profile_' + str(i) + '.json')\n",
    "    \n",
    "    for f in random_file_list:\n",
    "        print(f)\n",
    "        s3key = s3_load_prefix + f\n",
    "        obj = s3.Object(s3_bucket, s3key)\n",
    "        profile_data = obj.get()['Body'].read().decode('utf-8')\n",
    "        \n",
    "        #s3_profile_list = json.loads(profile_data)\n",
    "        stringIO_data = io.StringIO(profile_data)\n",
    "        data = stringIO_data.readlines()\n",
    "\n",
    "        #Its time to use json module now.\n",
    "        json_data = list(map(json.loads, data))\n",
    "\n",
    "        fakeIO = StringIO()\n",
    "        s3key = s3_update_prefix + f\n",
    "        fake_profile_data = []\n",
    "        \n",
    "        for rec in json_data:\n",
    "            # Let's generate a new address\n",
    "            print (\"old address: \" + rec['street_address'])\n",
    "            rec['street_address'] = fake.address()\n",
    "            print (\"new address: \" + rec['street_address'])\n",
    "            fake_profile_data.append(rec)\n",
    "            \n",
    "        fakeIO.write(str(''.join(dumps_lines(fake_profile_data))))\n",
    "        s3object = s3.Object(s3_bucket, s3key)\n",
    "        s3object.put(Body=(bytes(fakeIO.getvalue().encode('UTF-8'))))\n",
    "        fakeIO.close()\n",
    "\n",
    "def fake_profile_generator(length, fake, new_address=\"\"):\n",
    "    \"\"\"\n",
    "    Generates fake profiles\n",
    "    \"\"\"\n",
    "    for x in range (length):       \n",
    "        yield {'Name': fake.name(),\n",
    "               'phone': fake.phone_number(),\n",
    "               'job': fake.job(),\n",
    "               'company': fake.company(),\n",
    "               'ssn': fake.ssn(),\n",
    "               'street_address': (new_address if new_address else fake.address()),\n",
    "               'dob': (fake.date_of_birth(tzinfo=None, minimum_age=21, maximum_age=105).isoformat()),\n",
    "               'email': fake.email(),\n",
    "               'ts': (fake.date_time_between(start_date='-10y', end_date='now', tzinfo=None).isoformat()),\n",
    "               'credit_card': fake.credit_card_number(),\n",
    "               'record_id': fake.pyint(),\n",
    "               'id': fake.uuid4()}\n",
    "        \n",
    "def dumps_lines(objs):\n",
    "    for obj in objs:\n",
    "        yield json.dumps(obj, separators=(',',':')) + '\\n'   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the data generator\n",
    "\n",
    "Following code kicks off the fake data generator to produce files each with certain records (configurable) in JSON format. The files are written to a specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bulk_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Hudi Libraries on the EMR Cluster and create Hive table\n",
    "\n",
    "0. For the following steps to work, you should have launched the EMR cluster with appropriate permissions set for **Systems Manager Session Manager** \n",
    "1. From the AWS Console, type SSM in the search box and navigate to the **Amazon System Manager console**\n",
    "2. On the left hand side, select **Session Manager** from **Instances and Nodes** section\n",
    "3. Click on the start session and you should see two EC2 instances listed \n",
    "4. Select instance-id of the **EMR's Master** Node and click on **Start session**\n",
    "5. From the terminal type the following to change to user *ec2-user*\n",
    " \n",
    "```bash\n",
    "sh-4.2$ sudo su hadoop\n",
    "hadoop@ip-10-0-2-73 /]$ cd\n",
    "hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/jars/httpcore-4.4.11.jar /apps/hudi/lib/httpcore-4.4.11.jar\n",
    "hdfs dfs -ls /apps/hudi/lib/\n",
    "Found 5 items\n",
    "-rw-r--r--   1 hadoop hadoop     774384 2021-10-11 02:51 /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "-rw-r--r--   1 hadoop hadoop     326874 2021-10-11 02:51 /apps/hudi/lib/httpcore-4.4.11.jar\n",
    "-rw-r--r--   1 hadoop hadoop   35041795 2021-10-11 02:51 /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop   39996793 2021-10-11 02:51 /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop     161984 2021-10-11 02:51 /apps/hudi/lib/spark-avro.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to write a Copy on Write (COW) table\n",
    "\n",
    "We will now run the DeltaStreamer utility as an EMR Step to write the above JSON formatted data into a Hudi dataset. To do that, we will need the following:\n",
    "\n",
    "* Properties file on localfs or dfs, with configurations for Hudi client, schema provider, key generator and data source \n",
    "* Schema file for source dataset\n",
    "* Schema file for target dataset\n",
    "\n",
    "To run DeltaStreamer\n",
    "\n",
    "```\n",
    "! ~/.local/bin/aws emr add-steps --cluster-id j-1GMG9EJ4Z4ZL0 --steps Type=Spark,Name=\"Deltastreamer COW - Bulk Insert\",ActionOnFailure=CONTINUE,Args=[--jars,hdfs:///apps/hudi/lib/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://my-bucket/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://my-bucket/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Replace the following values in the above command in the text editor\n",
    "\n",
    "1. --cluster-id with the value you got from previous step\n",
    "2. For --props value replace xxxx part in hudi-workshop-xxxx with the S3 bucket name \n",
    "3. For -- target-base-path value with the S3 bucket name\n",
    "4. After replacing the values, copy the entire commmand and run it in the next cell\n",
    "5. If the values are replaced correctly, you should see a step id displayed as the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: awscli in ./.local/lib/python3.7/site-packages (1.20.58)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.6.0,>=0.5.0 in ./.local/lib/python3.7/site-packages (from awscli) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in ./.local/lib/python3.7/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: colorama<0.4.4,>=0.2.5 in ./.local/lib/python3.7/site-packages (from awscli) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML<5.5,>=3.10 in ./.local/lib/python3.7/site-packages (from awscli) (5.4.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore==1.21.58 in ./.local/lib/python3.7/site-packages (from awscli) (1.21.58)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.8,>=3.1.2 in /opt/conda/lib/python3.7/site-packages (from awscli) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore==1.21.58->awscli) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.21.58->awscli) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore==1.21.58->awscli) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.21.58->awscli) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install awscli --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/bin/aws\r\n"
     ]
    }
   ],
   "source": [
    "! ls ~/.local/bin/aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"StepIds\": [\r\n",
      "        \"s-22UV4POBZ7NZQ\"\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! ~/.local/bin/aws emr add-steps --cluster-id j-1GMG9EJ4Z4ZL0 --steps Type=Spark,Name=\"Deltastreamer COW - Bulk Insert\",ActionOnFailure=CONTINUE,Args=[--jars,hdfs:///apps/hudi/lib/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://vasveena-test-demo/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://vasveena-test-demo/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,BULK_INSERT] --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check the S3 path:\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<my bucket>/hudi-ds-output/person-profile-out1/\n",
    "                           PRE .hoodie/\n",
    "2021-10-11 02:55:31          0 .hoodie_$folder$\n",
    "2021-10-11 02:55:53         93 .hoodie_partition_metadata\n",
    "2021-10-11 02:55:55    2121824 37d44679-8b9a-4f53-864d-d3efe81b538b-0_2-4-34_20211011025540.parquet\n",
    "2021-10-11 02:55:55    1955015 4fefd569-0e95-4457-b6cb-ef49a96549f1-0_7-4-39_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2132101 50d1f972-f25c-47ee-a381-28a2d600b028-0_8-4-40_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2186257 574f9dcb-1d46-4357-8d1f-8c5145cf6351-0_0-4-32_20211011025540.parquet\n",
    "2021-10-11 02:55:55    1805544 b26e465b-2238-4ba7-a603-86105b42e5c9-0_6-4-38_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2479839 b7b62846-fe0d-4836-9146-4a2c412c4fa9-0_3-4-35_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2353476 d68bb0f1-4c72-45ec-8cd0-77470f8b2826-0_4-4-36_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2540438 ebbc234b-d953-4857-8f22-ff792bd919ed-0_9-4-41_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2984486 f49805bb-ccd0-446d-b1bc-36a12bb9a1fe-0_1-4-33_20211011025540.parquet\n",
    "2021-10-11 02:55:55    2261358 f8e3b197-eb7a-4a83-8d42-2e68757ff199-0_5-4-37_20211011025540.parquet\n",
    "```\n",
    "\n",
    "To query the Hudi dataset you can do one of the following\n",
    "\n",
    "- Navigate to the another sparkmagic notebook and run queries in Spark using SparkMagic cell\n",
    "- SSH to the master node (you can also SSM if you launched your cluster with SSM permissions) and run queries using Hive/Presto\n",
    "- Head to the Hue console on Amazon EMR and run queries\n",
    "- Query using Amazon Athena or Redshift spectrum (preferred)\n",
    "\n",
    "Let us use Athena to query\n",
    "\n",
    "```\n",
    "\n",
    "In Athena console: \n",
    "\n",
    "CREATE EXTERNAL TABLE `profile_cow`(\n",
    "  `_hoodie_commit_time` string, \n",
    "  `_hoodie_commit_seqno` string, \n",
    "  `_hoodie_record_key` string, \n",
    "  `_hoodie_partition_path` string, \n",
    "  `_hoodie_file_name` string, \n",
    "  `Name` string, \n",
    "  `phone` string, \n",
    "  `job` string, \n",
    "  `company` string, \n",
    "  `ssn` string, \n",
    "  `street_address` string, \n",
    "  `dob` string, \n",
    "  `email` string, \n",
    "  `ts` string)\n",
    "ROW FORMAT SERDE \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hudi.hadoop.HoodieParquetInputFormat' \n",
    "OUTPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
    "LOCATION\n",
    "  's3://my-bucket/hudi-ds-output/person-profile-out1';\n",
    "\n",
    "select * from profile_cow limit 2;\n",
    "\n",
    "\t_hoodie_commit_time\t_hoodie_commit_seqno\t_hoodie_record_key\t_hoodie_partition_path\t_hoodie_file_name\tname\tphone\tjob\tcompany\tssn\tstreet_address\tdob\temail\tts\n",
    "1\t20211011025540\t20211011025540_2_1\t3b288239-a6cf-4312-8044-cd984ddd1066\t\t37d44679-8b9a-4f53-864d-d3efe81b538b-0_2-4-34_20211011025540.parquet\tDavid Foley\t001-798-549-5064\tClinical psychologist\tHays LLC\t711-86-5759\t941 Charles Centers New Christina, WY 22122\t1942-07-27\thwalsh@example.com\t2012-02-22T04:08:36\n",
    "2\t20211011025540\t20211011025540_2_2\t3b28bd5c-d5fc-4694-837f-58fb62289775\t\t37d44679-8b9a-4f53-864d-d3efe81b538b-0_2-4-34_20211011025540.parquet\tEric Woods\t001-655-150-8537x4698\tArchaeologist\tHenry Ltd\t394-48-5755\t93277 Laurie Trail Suite 421 Crawfordmouth, WY 86413\t1954-11-23\tgrayjennifer@example.net\t2017-08-27T00:06:25\n",
    "\n",
    "Now, lets make a note of street_address in one of these two records -> \"941 Charles Centers New Christina, WY 22122\"\n",
    "\n",
    "select _hoodie_commit_time, street_address from profile_cow where _hoodie_record_key='3b288239-a6cf-4312-8044-cd984ddd1066';\n",
    "\n",
    "_hoodie_commit_time\tstreet_address\n",
    "1\t20211011025540\t941 Charles Centers New Christina, WY 22122  \n",
    "\n",
    "```\n",
    "\n",
    "Lets now run an upsert to observe the change in records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_updates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the records in updates/ location.\n",
    "\n",
    "```\n",
    "$ $ aws s3 ls s3://vasveena-test-demo/hudi-ds/updates/\n",
    "2021-10-11 03:56:03    3686203 profile_1.json\n",
    "2021-10-11 03:56:09    3688069 profile_2.json\n",
    "2021-10-11 03:56:14    3683899 profile_3.json\n",
    "2021-10-11 03:56:19    3684258 profile_4.json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to apply updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the Deltastreamer again to run upserts using the updates generated in the previous step.\n",
    "\n",
    "```\n",
    "\n",
    "! ~/.local/bin/aws emr add-steps --cluster-id j-XXXXXXX --steps Type=Spark,Name=\"Deltastreamer Profile Upserts\",ActionOnFailure=CONTINUE,Args=[--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://<my-bucket>/hudi-ds/config/json-deltastreamer.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://<my-bucket>/hudi-ds/output/profile-test15-out,--target-table,profile_test15_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"StepIds\": [\r\n",
      "        \"s-110EI3UO5BVNR\"\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! ~/.local/bin/aws emr add-steps --cluster-id j-1GMG9EJ4Z4ZL0 --steps Type=Spark,Name=\"Deltastreamer COW\",ActionOnFailure=CONTINUE,Args=[--jars,hdfs:///apps/hudi/lib/*.jar,--class,org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer,hdfs:///apps/hudi/lib/hudi-utilities-bundle.jar,--props,s3://vasveena-test-demo/config/json-deltastreamer_upsert.properties,--table-type,COPY_ON_WRITE,--source-class,org.apache.hudi.utilities.sources.JsonDFSSource,--source-ordering-field,ts,--target-base-path,s3://vasveena-test-demo/hudi-ds-output/person-profile-out1,--target-table,person_profile_cow,--schemaprovider-class,org.apache.hudi.utilities.schema.FilebasedSchemaProvider,--op,UPSERT] --region us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the updated Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the S3 path of output location. Notice the new Parquet files. \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<my-bucket>/hudi-ds-output/person-profile-out1/\n",
    "                           PRE .hoodie/\n",
    "2021-10-11 02:55:31          0 .hoodie_$folder$\n",
    "2021-10-11 02:55:53         93 .hoodie_partition_metadata\n",
    "2021-10-11 03:58:38    2124844 37d44679-8b9a-4f53-864d-d3efe81b538b-0_2-22-130_20211011035814.parquet\n",
    "2021-10-11 02:55:55    2121824 37d44679-8b9a-4f53-864d-d3efe81b538b-0_2-4-34_20211011025540.parquet\n",
    "2021-10-11 02:55:55    1955015 4fefd569-0e95-4457-b6cb-ef49a96549f1-0_7-4-39_20211011025540.parquet\n",
    "2021-10-11 03:58:38    1958284 4fefd569-0e95-4457-b6cb-ef49a96549f1-0_9-22-137_20211011035814.parquet\n",
    "2021-10-11 03:58:38    2135767 50d1f972-f25c-47ee-a381-28a2d600b028-0_0-22-128_20211011035814.parquet\n",
    "2021-10-11 02:55:56    2132101 50d1f972-f25c-47ee-a381-28a2d600b028-0_8-4-40_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2186257 574f9dcb-1d46-4357-8d1f-8c5145cf6351-0_0-4-32_20211011025540.parquet\n",
    "2021-10-11 03:58:38    2189874 574f9dcb-1d46-4357-8d1f-8c5145cf6351-0_7-22-135_20211011035814.parquet\n",
    "2021-10-11 03:58:39    1808741 b26e465b-2238-4ba7-a603-86105b42e5c9-0_1-22-129_20211011035814.parquet\n",
    "2021-10-11 02:55:55    1805544 b26e465b-2238-4ba7-a603-86105b42e5c9-0_6-4-38_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2479839 b7b62846-fe0d-4836-9146-4a2c412c4fa9-0_3-4-35_20211011025540.parquet\n",
    "2021-10-11 03:58:39    2484653 b7b62846-fe0d-4836-9146-4a2c412c4fa9-0_4-22-132_20211011035814.parquet\n",
    "2021-10-11 02:55:56    2353476 d68bb0f1-4c72-45ec-8cd0-77470f8b2826-0_4-4-36_20211011025540.parquet\n",
    "2021-10-11 03:58:39    2358152 d68bb0f1-4c72-45ec-8cd0-77470f8b2826-0_6-22-134_20211011035814.parquet\n",
    "2021-10-11 03:58:39    2544868 ebbc234b-d953-4857-8f22-ff792bd919ed-0_8-22-136_20211011035814.parquet\n",
    "2021-10-11 02:55:56    2540438 ebbc234b-d953-4857-8f22-ff792bd919ed-0_9-4-41_20211011025540.parquet\n",
    "2021-10-11 02:55:56    2984486 f49805bb-ccd0-446d-b1bc-36a12bb9a1fe-0_1-4-33_20211011025540.parquet\n",
    "2021-10-11 03:58:40    2989781 f49805bb-ccd0-446d-b1bc-36a12bb9a1fe-0_3-22-131_20211011035814.parquet\n",
    "2021-10-11 03:58:39    2265328 f8e3b197-eb7a-4a83-8d42-2e68757ff199-0_5-22-133_20211011035814.parquet\n",
    "2021-10-11 02:55:55    2261358 f8e3b197-eb7a-4a83-8d42-2e68757ff199-0_5-4-37_20211011025540.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Let's query an upserted record. \n",
    "\n",
    "```\n",
    "select _hoodie_commit_time, street_address from profile_cow where _hoodie_record_key='3b288239-a6cf-4312-8044-cd984ddd1066';\n",
    "\n",
    "20211011025540    941 Charles Centers New Christina, WY 22122               # Old address \n",
    "20211011035814\t35740 Young Orchard Suite 147 South Williamport, MT 82610   # Our recent update \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Now lets check out Hudi CLI\n",
    "\n",
    "```\n",
    "connect --path s3://<my-bucket>/hudi-ds-output/person-profile-out1/\n",
    "hudi:person_profile_cow->commits show\n",
    "2021-10-11 04:01:17,704 INFO timeline.HoodieActiveTimeline: Loaded instants [[20211011025540__commit__COMPLETED], [20211011035814__commit__COMPLETED]]\n",
    "2021-10-11 04:01:17,728 INFO s3n.S3NativeFileSystem: Opening 's3://vasveena-test-demo/hudi-ds-output/person-profile-out1/.hoodie/20211011035814.commit' for reading\n",
    "2021-10-11 04:01:18,044 INFO s3n.S3NativeFileSystem: Opening 's3://vasveena-test-demo/hudi-ds-output/person-profile-out1/.hoodie/20211011025540.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20211011035814 │ 21.8 MB             │ 0                 │ 10                  │ 1                        │ 150000                │ 140000                       │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20211011025540 │ 21.8 MB             │ 10                │ 0                   │ 1                        │ 150000                │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
