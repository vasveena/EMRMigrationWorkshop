{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d558b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer'}, 'proxyUser': 'assumed-role_TeamRole_MasterKey', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\", \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450a86c",
   "metadata": {},
   "source": [
    "You can also run the commands on Spark shell\n",
    "\n",
    "spark-shell --jars hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar --executor-memory=24544M --executor-cores=4 --conf spark.sql.hive.convertMetastoreParquet=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.dynamicAllocation.executorIdleTimeout=3600 --conf spark.dynamicAllocation.initialExecutors=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b642a493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9fa84021d040aebdd483654c0c77b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1637560469042_0022</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-0-73.ec2.internal:20888/proxy/application_1637560469042_0022/\" class=\"emr-proxy-link\" emr-resource=\"j-1HCZO7EIKLFQY\n",
       "\" application-id=\"application_1637560469042_0022\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-0-185.ec2.internal:8042/node/containerlogs/container_1637560469042_0022_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUDI_FORMAT: String = org.apache.hudi\n",
      "TABLE_NAME: String = hoodie.table.name\n",
      "RECORDKEY_FIELD_OPT_KEY: String = hoodie.datasource.write.recordkey.field\n",
      "PRECOMBINE_FIELD_OPT_KEY: String = hoodie.datasource.write.precombine.field\n",
      "OPERATION_OPT_KEY: String = hoodie.datasource.write.operation\n",
      "BULK_INSERT_OPERATION_OPT_VAL: String = bulk_insert\n",
      "UPSERT_OPERATION_OPT_VAL: String = upsert\n",
      "BULK_INSERT_PARALLELISM: String = hoodie.bulkinsert.shuffle.parallelism\n",
      "UPSERT_PARALLELISM: String = hoodie.upsert.shuffle.parallelism\n",
      "S3_CONSISTENCY_CHECK: String = hoodie.consistency.check.enabled\n",
      "HUDI_CLEANER_POLICY: String = hoodie.cleaner.policy\n",
      "KEEP_LATEST_COMMITS: String = KEEP_LATEST_COMMITS\n",
      "HUDI_COMMITS_RETAINED: String = hoodie.cleaner.commits.retained\n",
      "PAYLOAD_CLASS_OPT_KEY: String = hoodie.datasource.write.payload.class\n",
      "EMPTY_PAYLOAD_CLASS_OPT_VAL: String = org.apache.hudi.common.model.EmptyHoodieRecordPayload\n",
      "TABLE_TYPE_OPT_KEY: String = hoodie.datasource.write.table.type\n",
      "HIVE_SYNC_ENABLED_OPT_KEY: String = hoodie.datasource.hive_sync.enable\n",
      "HIVE_PARTITION_FIELDS_OPT_KEY: String = hoodie.datasource.hive_sync.partition_fields\n",
      "HIVE_ASSUME_DATE_PARTITION_OPT_KEY: String = hoodie.datasource.hive_sync.assume_date_partitioning\n",
      "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY: String = hoodie.datasource.hive_sync.partition_extractor_class\n",
      "HIVE_TABLE_OPT_KEY: String = hoodie.datasource.hive_sync.table\n",
      "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL: String = org.apache.hudi.hive.NonPartitionedExtractor\n",
      "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL: String = org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "KEYGENERATOR_CLASS_OPT_KEY: String = hoodie.datasource.write.keygenerator.class\n",
      "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL: String = org.apache.hudi.keygen.NonpartitionedKeyGenerator\n",
      "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL: String = org.apache.hudi.ComplexKeyGenerator\n",
      "PARTITIONPATH_FIELD_OPT_KEY: String = hoodie.datasource.write.partitionpath.field\n",
      "VIEW_TYPE_OPT_KEY: String = hoodie.datasource.view.type\n",
      "BEGIN_INSTANTTIME_OPT_KEY: String = hoodie.datasource.read.begin.instanttime\n",
      "VIEW_TYPE_INCREMENTAL_OPT_VAL: String = incremental\n",
      "END_INSTANTTIME_OPT_KEY: String = hoodie.datasource.read.end.instanttime\n"
     ]
    }
   ],
   "source": [
    "// General Constants\n",
    "val HUDI_FORMAT = \"org.apache.hudi\"\n",
    "val TABLE_NAME = \"hoodie.table.name\"\n",
    "val RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "val PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "val OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "val BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "val UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "val BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "val UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "val S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "val HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "val KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "val HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "val PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "val EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "val TABLE_TYPE_OPT_KEY=\"hoodie.datasource.write.table.type\"\n",
    "\n",
    "// Hive Constants\n",
    "val HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "val HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "val HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "val HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "val HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "// Partition Constants\n",
    "val NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "val MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "val KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "val NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "val COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "val PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "//Incremental Constants\n",
    "val VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "val BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "val VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "val END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8527da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2096550cec450198e124935e37c487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n",
      "import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}\n",
      "import org.apache.spark.sql.ForeachWriter\n",
      "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.streaming._\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}\n",
      "import java.util.HashMap\n",
      "import spark.implicits._\n",
      "import org.apache.hudi._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n",
    "import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}\n",
    "import java.util.HashMap\n",
    "import spark.implicits._\n",
    "import org.apache.hudi._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459f2291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667921fd5bad4e329ddd881edb28c7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_update_topic: String = trip_update_topic\n",
      "trip_status_topic: String = trip_status_topic\n",
      "broker: String = b-3.mskcluster.4ihigb.c20.kafka.us-east-1.amazonaws.com:9092,b-2.mskcluster.4ihigb.c20.kafka.us-east-1.amazonaws.com:9092,b-1.mskcluster.4ihigb.c20.kafka.us-east-1.amazonaws.com:9092\n"
     ]
    }
   ],
   "source": [
    "// CHANGE ME \n",
    "\n",
    "val trip_update_topic = \"trip_update_topic\"\n",
    "val trip_status_topic = \"trip_status_topic\"\n",
    "//Change broker string with value obtained from CLI command \n",
    "val broker = \"b-3.mskcluster.4ihigb.c20.kafka.us-east-1.amazonaws.com:9092,b-2.mskcluster.4ihigb.c20.kafka.us-east-1.amazonaws.com:9092,b-1.mskcluster.4ihigb.c20.kafka.us-east-1.amazonaws.com:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7256c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a632174a78f54d6c96ad617ced72d844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined object MTASubwayTripUpdates\n"
     ]
    }
   ],
   "source": [
    "// CHANGE ME\n",
    "\n",
    "// Replace S3 location\n",
    "\n",
    "object MTASubwayTripUpdates extends Serializable {\n",
    "\n",
    "    val props = new HashMap[String, Object]()\n",
    "    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker)\n",
    "    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "    @transient var producer : KafkaProducer[String, String] = null\n",
    "    var msgId : Long = 1\n",
    "    @transient var joined_query : StreamingQuery = null\n",
    "    @transient var joined_query_s3 : StreamingQuery = null\n",
    "\n",
    "    val spark = SparkSession.builder.appName(\"MSK streaming Example\").getOrCreate()\n",
    "    \n",
    "\n",
    "    def start() = {\n",
    "        //Start producer for kafka\n",
    "        producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "        //Create a datastream from trip update topic\n",
    "        val trip_update_df = spark.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", broker)\n",
    "        .option(\"subscribe\", trip_update_topic)\n",
    "        .option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load()\n",
    "\n",
    "        //Create a datastream from trip status topic\n",
    "        val trip_status_df = spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", broker)\n",
    "        .option(\"subscribe\", trip_status_topic)\n",
    "        .option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load()\n",
    "\n",
    "        // define schema of data\n",
    "\n",
    "        val trip_update_schema = new StructType()\n",
    "        .add(\"trip\", new StructType().add(\"tripId\",\"string\").add(\"startTime\",\"string\").add(\"startDate\",\"string\").add(\"routeId\",\"string\"))\n",
    "        .add(\"stopTimeUpdate\",ArrayType(new StructType().add(\"arrival\",new StructType().add(\"time\",\"string\")).add(\"stopId\",\"string\").add(\"departure\",new StructType().add(\"time\",\"string\"))))\n",
    "\n",
    "        val trip_status_schema = new StructType()\n",
    "        .add(\"trip\", new StructType().add(\"tripId\",\"string\").add(\"startTime\",\"string\").add(\"startDate\",\"string\").add(\"routeId\",\"string\")).add(\"currentStopSequence\",\"integer\").add(\"currentStatus\", \"string\").add(\"timestamp\", \"string\").add(\"stopId\",\"string\")\n",
    "\n",
    "        // covert datastream into a datasets and apply schema\n",
    "        val trip_update_ds = trip_update_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n",
    "        val trip_update_ds_schema = trip_update_ds\n",
    "        .select(from_json($\"value\", trip_update_schema).as(\"data\")).select(\"data.*\")\n",
    "        trip_update_ds_schema.printSchema()\n",
    "\n",
    "        val trip_status_ds = trip_status_df\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n",
    "        val trip_status_ds_schema = trip_status_ds\n",
    "        .select(from_json($\"value\", trip_status_schema).as(\"data\")).select(\"data.*\")\n",
    "        trip_status_ds_schema.printSchema()\n",
    "\n",
    "        val trip_status_ds_unnest = trip_status_ds_schema\n",
    "        .select(\"trip.*\",\"currentStopSequence\",\"currentStatus\",\"timestamp\",\"stopId\")\n",
    "\n",
    "        val trip_update_ds_unnest = trip_update_ds_schema\n",
    "        .select($\"trip.*\", $\"stopTimeUpdate.arrival.time\".as(\"arrivalTime\"), \n",
    "                $\"stopTimeUpdate.departure.time\".as(\"depatureTime\"), $\"stopTimeUpdate.stopId\")\n",
    "\n",
    "        val trip_update_ds_unnest2 = trip_update_ds_unnest\n",
    "        .withColumn(\"numOfFutureStops\", size($\"arrivalTime\"))\n",
    "        .withColumnRenamed(\"stopId\",\"futureStopIds\")\n",
    "\n",
    "        val joined_ds = trip_update_ds_unnest2\n",
    "        .join(trip_status_ds_unnest, Seq(\"tripId\",\"routeId\",\"startTime\",\"startDate\"))\n",
    "        .withColumn(\"startTime\",(col(\"startTime\").cast(\"timestamp\")))\n",
    "        .withColumn(\"currentTs\",from_unixtime($\"timestamp\".divide(1000)))\n",
    "        .drop(\"startDate\").drop(\"timestamp\")\n",
    "\n",
    "        joined_ds.printSchema()\n",
    "\n",
    "        //console\n",
    "        val joined_query = joined_ds\n",
    "        .writeStream.outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", \"true\")\n",
    "        .outputMode(OutputMode.Append()).trigger(Trigger.ProcessingTime(\"10 seconds\")).start()\n",
    "        \n",
    "        def myFunc( batchDF:DataFrame, batchID:Long ) : Unit = {\n",
    "            batchDF.persist()\n",
    "            \n",
    "            batchDF.write.format(\"org.apache.hudi\")\n",
    "                .option(TABLE_TYPE_OPT_KEY, \"COPY_ON_WRITE\")\n",
    "                .option(PRECOMBINE_FIELD_OPT_KEY, \"currentTs\")\n",
    "                .option(RECORDKEY_FIELD_OPT_KEY, \"tripId\")\n",
    "                .option(TABLE_NAME, \"hudi_trips_streaming_table\")\n",
    "                .option(UPSERT_PARALLELISM, 200)\n",
    "                .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "                .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "                .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "                .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "                .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "                .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save(\"s3://emr-workshop-197690357257-template/demos/hudi_streaming_output/\")\n",
    "            \n",
    "            batchDF.unpersist()\n",
    "        }\n",
    "        \n",
    "        val query = joined_ds.writeStream.queryName(\"lab3\")\n",
    "        .trigger(Trigger.ProcessingTime(\"60 seconds\"))\n",
    "        .foreachBatch(myFunc _)\n",
    "    \n",
    "      .option(\"checkpointLocation\", \"/user/hadoop/checkpoint\")\n",
    "      .start()\n",
    "        \n",
    "      query.awaitTermination()\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0151d219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ca720bcffd4aa68b762d9bcfea42ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b5fb065b074f578006b5fcda1a9c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-58e83c24f6df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MTASubwayTripUpdates.start\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2401\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2404\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce, cell_kind)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_kind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmimetype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_kind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_session_on_spark_statement_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MTASubwayTripUpdates.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4970f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
