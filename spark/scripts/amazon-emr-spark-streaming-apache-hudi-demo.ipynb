{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '24544M', 'spark.executor.cores': 4, 'spark.dynamicAllocation.initialExecutors': 8}, 'proxyUser': 'jovyan', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1633720478106_0058</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-42-191.ec2.internal:20888/proxy/application_1633720478106_0058/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-39-218.ec2.internal:8042/node/containerlogs/container_1633720478106_0058_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\", \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"24544M\",\n",
    "             \"spark.executor.cores\": 4,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":8\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the commands on Spark shell\n",
    "\n",
    "spark-shell --jars hdfs:///user/hadoop/httpcore-4.4.11.jar,hdfs:///user/hadoop/httpclient-4.5.9.jar,hdfs:///user/hadoop/hudi-spark-bundle.jar,hdfs:///user/hadoop/spark-avro.jar --executor-memory=24544M --executor-cores=4 --conf spark.sql.hive.convertMetastoreParquet=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.dynamicAllocation.executorIdleTimeout=3600 --conf spark.dynamicAllocation.initialExecutors=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1633720478106_0062</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-42-191.ec2.internal:20888/proxy/application_1633720478106_0062/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-40-249.ec2.internal:8042/node/containerlogs/container_1633720478106_0062_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUDI_FORMAT: String = org.apache.hudi\n",
      "TABLE_NAME: String = hoodie.table.name\n",
      "RECORDKEY_FIELD_OPT_KEY: String = hoodie.datasource.write.recordkey.field\n",
      "PRECOMBINE_FIELD_OPT_KEY: String = hoodie.datasource.write.precombine.field\n",
      "OPERATION_OPT_KEY: String = hoodie.datasource.write.operation\n",
      "BULK_INSERT_OPERATION_OPT_VAL: String = bulk_insert\n",
      "UPSERT_OPERATION_OPT_VAL: String = upsert\n",
      "BULK_INSERT_PARALLELISM: String = hoodie.bulkinsert.shuffle.parallelism\n",
      "UPSERT_PARALLELISM: String = hoodie.upsert.shuffle.parallelism\n",
      "S3_CONSISTENCY_CHECK: String = hoodie.consistency.check.enabled\n",
      "HUDI_CLEANER_POLICY: String = hoodie.cleaner.policy\n",
      "KEEP_LATEST_COMMITS: String = KEEP_LATEST_COMMITS\n",
      "HUDI_COMMITS_RETAINED: String = hoodie.cleaner.commits.retained\n",
      "PAYLOAD_CLASS_OPT_KEY: String = hoodie.datasource.write.payload.class\n",
      "EMPTY_PAYLOAD_CLASS_OPT_VAL: String = org.apache.hudi.common.model.EmptyHoodieRecordPayload\n",
      "STORAGE_TYPE_OPT_KEY: String = hoodie.datasource.write.storage.type\n",
      "HIVE_SYNC_ENABLED_OPT_KEY: String = hoodie.datasource.hive_sync.enable\n",
      "HIVE_PARTITION_FIELDS_OPT_KEY: String = hoodie.datasource.hive_sync.partition_fields\n",
      "HIVE_ASSUME_DATE_PARTITION_OPT_KEY: String = hoodie.datasource.hive_sync.assume_date_partitioning\n",
      "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY: String = hoodie.datasource.hive_sync.partition_extractor_class\n",
      "HIVE_TABLE_OPT_KEY: String = hoodie.datasource.hive_sync.table\n",
      "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL: String = org.apache.hudi.hive.NonPartitionedExtractor\n",
      "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL: String = org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
      "KEYGENERATOR_CLASS_OPT_KEY: String = hoodie.datasource.write.keygenerator.class\n",
      "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL: String = org.apache.hudi.keygen.NonpartitionedKeyGenerator\n",
      "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL: String = org.apache.hudi.ComplexKeyGenerator\n",
      "PARTITIONPATH_FIELD_OPT_KEY: String = hoodie.datasource.write.partitionpath.field\n",
      "VIEW_TYPE_OPT_KEY: String = hoodie.datasource.view.type\n",
      "BEGIN_INSTANTTIME_OPT_KEY: String = hoodie.datasource.read.begin.instanttime\n",
      "VIEW_TYPE_INCREMENTAL_OPT_VAL: String = incremental\n",
      "END_INSTANTTIME_OPT_KEY: String = hoodie.datasource.read.end.instanttime\n"
     ]
    }
   ],
   "source": [
    "// General Constants\n",
    "val HUDI_FORMAT = \"org.apache.hudi\"\n",
    "val TABLE_NAME = \"hoodie.table.name\"\n",
    "val RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "val PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "val OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "val BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "val UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "val BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\"\n",
    "val UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "val S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "val HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "val KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "val HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "val PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "val EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "val TABLE_TYPE_OPT_KEY=\"hoodie.datasource.write.table.type\"\n",
    "\n",
    "// Hive Constants\n",
    "val HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "val HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "val HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "val HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "val HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "// Partition Constants\n",
    "val NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "val MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "val KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "val NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "val COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "val PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "//Incremental Constants\n",
    "val VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "val BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "val VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "val END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n",
      "import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}\n",
      "import org.apache.spark.sql.ForeachWriter\n",
      "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.streaming._\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}\n",
      "import java.util.HashMap\n",
      "import spark.implicits._\n",
      "import org.apache.hudi._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n",
    "import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.kafka.clients.producer.{ProducerConfig, KafkaProducer, ProducerRecord}\n",
    "import java.util.HashMap\n",
    "import spark.implicits._\n",
    "import org.apache.hudi._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_update_topic: String = trip_update_topic\n",
      "trip_status_topic: String = trip_status_topic\n",
      "broker: String = b-3.test.1tklkx.c2.kafka.us-east-1.amazonaws.com:9092,b-1.test.1tklkx.c2.kafka.us-east-1.amazonaws.com:9092,b-2.test.1tklkx.c2.kafka.us-east-1.amazonaws.com:9092\n"
     ]
    }
   ],
   "source": [
    "val trip_update_topic = \"trip_update_topic\"\n",
    "val trip_status_topic = \"trip_status_topic\"\n",
    "val broker = \"b-3.test.1tklkx.c2.kafka.us-east-1.amazonaws.com:9092,b-1.test.1tklkx.c2.kafka.us-east-1.amazonaws.com:9092,b-2.test.1tklkx.c2.kafka.us-east-1.amazonaws.com:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined object MTASubwayTripUpdates\n"
     ]
    }
   ],
   "source": [
    "object MTASubwayTripUpdates extends Serializable {\n",
    "\n",
    "    val props = new HashMap[String, Object]()\n",
    "    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker)\n",
    "    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "    @transient var producer : KafkaProducer[String, String] = null\n",
    "    var msgId : Long = 1\n",
    "    @transient var joined_query : StreamingQuery = null\n",
    "    @transient var joined_query_s3 : StreamingQuery = null\n",
    "\n",
    "    val spark = SparkSession.builder.appName(\"MSK streaming Example\").getOrCreate()\n",
    "    \n",
    "\n",
    "    def start() = {\n",
    "        //Start producer for kafka\n",
    "        producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "        //Create a datastream from trip update topic\n",
    "        val trip_update_df = spark.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", broker)\n",
    "        .option(\"subscribe\", trip_update_topic)\n",
    "        .option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load()\n",
    "\n",
    "        //Create a datastream from trip status topic\n",
    "        val trip_status_df = spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", broker)\n",
    "        .option(\"subscribe\", trip_status_topic)\n",
    "        .option(\"startingOffsets\", \"latest\").option(\"failOnDataLoss\",\"false\").load()\n",
    "\n",
    "        // define schema of data\n",
    "\n",
    "        val trip_update_schema = new StructType()\n",
    "        .add(\"trip\", new StructType().add(\"tripId\",\"string\").add(\"startTime\",\"string\").add(\"startDate\",\"string\").add(\"routeId\",\"string\"))\n",
    "        .add(\"stopTimeUpdate\",ArrayType(new StructType().add(\"arrival\",new StructType().add(\"time\",\"string\")).add(\"stopId\",\"string\").add(\"departure\",new StructType().add(\"time\",\"string\"))))\n",
    "\n",
    "        val trip_status_schema = new StructType()\n",
    "        .add(\"trip\", new StructType().add(\"tripId\",\"string\").add(\"startTime\",\"string\").add(\"startDate\",\"string\").add(\"routeId\",\"string\")).add(\"currentStopSequence\",\"integer\").add(\"currentStatus\", \"string\").add(\"timestamp\", \"string\").add(\"stopId\",\"string\")\n",
    "\n",
    "        // covert datastream into a datasets and apply schema\n",
    "        val trip_update_ds = trip_update_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n",
    "        val trip_update_ds_schema = trip_update_ds\n",
    "        .select(from_json($\"value\", trip_update_schema).as(\"data\")).select(\"data.*\")\n",
    "        trip_update_ds_schema.printSchema()\n",
    "\n",
    "        val trip_status_ds = trip_status_df\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n",
    "        val trip_status_ds_schema = trip_status_ds\n",
    "        .select(from_json($\"value\", trip_status_schema).as(\"data\")).select(\"data.*\")\n",
    "        trip_status_ds_schema.printSchema()\n",
    "\n",
    "        val trip_status_ds_unnest = trip_status_ds_schema\n",
    "        .select(\"trip.*\",\"currentStopSequence\",\"currentStatus\",\"timestamp\",\"stopId\")\n",
    "\n",
    "        val trip_update_ds_unnest = trip_update_ds_schema\n",
    "        .select($\"trip.*\", $\"stopTimeUpdate.arrival.time\".as(\"arrivalTime\"), \n",
    "                $\"stopTimeUpdate.departure.time\".as(\"depatureTime\"), $\"stopTimeUpdate.stopId\")\n",
    "\n",
    "        val trip_update_ds_unnest2 = trip_update_ds_unnest\n",
    "        .withColumn(\"numOfFutureStops\", size($\"arrivalTime\"))\n",
    "        .withColumnRenamed(\"stopId\",\"futureStopIds\")\n",
    "\n",
    "        val joined_ds = trip_update_ds_unnest2\n",
    "        .join(trip_status_ds_unnest, Seq(\"tripId\",\"routeId\",\"startTime\",\"startDate\"))\n",
    "        .withColumn(\"startTime\",(col(\"startTime\").cast(\"timestamp\")))\n",
    "        .withColumn(\"currentTs\",from_unixtime($\"timestamp\".divide(1000)))\n",
    "        .drop(\"startDate\").drop(\"timestamp\")\n",
    "\n",
    "        joined_ds.printSchema()\n",
    "\n",
    "        //console\n",
    "        val joined_query = joined_ds\n",
    "        .writeStream.outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", \"true\")\n",
    "        .outputMode(OutputMode.Append()).trigger(Trigger.ProcessingTime(\"10 seconds\")).start()\n",
    "        \n",
    "        def myFunc( batchDF:DataFrame, batchID:Long ) : Unit = {\n",
    "            batchDF.persist()\n",
    "            \n",
    "            batchDF.write.format(\"org.apache.hudi\")\n",
    "                .option(TABLE_TYPE_OPT_KEY, \"COPY_ON_WRITE\")\n",
    "                .option(PRECOMBINE_FIELD_OPT_KEY, \"currentTs\")\n",
    "                .option(RECORDKEY_FIELD_OPT_KEY, \"tripId\")\n",
    "                .option(TABLE_NAME, \"hudi_trips_streaming_table\")\n",
    "                .option(UPSERT_PARALLELISM, 200)\n",
    "                .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "                .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "                .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "                .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "                .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "                .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "                .mode(SaveMode.Append)\n",
    "                .save(\"s3://vasveena-test-demo/demos/streaming_output/\")\n",
    "            \n",
    "            batchDF.unpersist()\n",
    "        }\n",
    "        \n",
    "        val query = joined_ds.writeStream.queryName(\"lab3\")\n",
    "        .trigger(Trigger.ProcessingTime(\"60 seconds\"))\n",
    "        .foreachBatch(myFunc _)\n",
    "    \n",
    "      .option(\"checkpointLocation\", \"/user/hadoop/checkpoint\")\n",
    "      .start()\n",
    "        \n",
    "      query.awaitTermination()\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "root\n",
      " |-- trip: struct (nullable = true)\n",
      " |    |-- tripId: string (nullable = true)\n",
      " |    |-- startTime: string (nullable = true)\n",
      " |    |-- startDate: string (nullable = true)\n",
      " |    |-- routeId: string (nullable = true)\n",
      " |-- stopTimeUpdate: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- arrival: struct (nullable = true)\n",
      " |    |    |    |-- time: string (nullable = true)\n",
      " |    |    |-- stopId: string (nullable = true)\n",
      " |    |    |-- departure: struct (nullable = true)\n",
      " |    |    |    |-- time: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- trip: struct (nullable = true)\n",
      " |    |-- tripId: string (nullable = true)\n",
      " |    |-- startTime: string (nullable = true)\n",
      " |    |-- startDate: string (nullable = true)\n",
      " |    |-- routeId: string (nullable = true)\n",
      " |-- currentStopSequence: integer (nullable = true)\n",
      " |-- currentStatus: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- stopId: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- tripId: string (nullable = true)\n",
      " |-- routeId: string (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      " |-- arrivalTime: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- depatureTime: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- futureStopIds: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- numOfFutureStops: integer (nullable = false)\n",
      " |-- currentStopSequence: integer (nullable = true)\n",
      " |-- currentStatus: string (nullable = true)\n",
      " |-- stopId: string (nullable = true)\n",
      " |-- currentTs: string (nullable = true)\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-------+---------+-----------+------------+-------------+----------------+-------------------+-------------+------+---------+\n",
      "|tripId|routeId|startTime|arrivalTime|depatureTime|futureStopIds|numOfFutureStops|currentStopSequence|currentStatus|stopId|currentTs|\n",
      "+------+-------+---------+-----------+------------+-------------+----------------+-------------------+-------------+------+---------+\n",
      "+------+-------+---------+-----------+------------+-------------+----------------+-------------------+-------------+------+---------+\n",
      "\n",
      "org.apache.spark.sql.streaming.StreamingQueryException: Query lab3 [id = 5c27765e-cde8-42a5-b721-4785a63ec2a9, runId = 9bd4fdbf-d64e-4d89-8605-6d26975e99ce] terminated with exception: Job aborted due to stage failure: Task 0 in stage 38.0 failed 4 times, most recent failure: Lost task 0.3 in stage 38.0 (TID 745, ip-172-31-39-218.ec2.internal, executor 8): java.lang.NoSuchMethodError: org.apache.spark.kafka010.KafkaTokenUtil$.needTokenUpdate(Lorg/apache/spark/SparkConf;Ljava/util/Map;Lscala/Option;)Z\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getOrRetrieveConsumer(KafkaDataConsumer.scala:532)\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$get$1(KafkaDataConsumer.scala:274)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:587)\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.get(KafkaDataConsumer.scala:270)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaBatchPartitionReader.next(KafkaBatchPartitionReader.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:133)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:355)\n",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 4 times, most recent failure: Lost task 0.3 in stage 38.0 (TID 745, ip-172-31-39-218.ec2.internal, executor 8): java.lang.NoSuchMethodError: org.apache.spark.kafka010.KafkaTokenUtil$.needTokenUpdate(Lorg/apache/spark/SparkConf;Ljava/util/Map;Lscala/Option;)Z\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getOrRetrieveConsumer(KafkaDataConsumer.scala:532)\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$get$1(KafkaDataConsumer.scala:274)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:587)\n",
      "\tat org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.get(KafkaDataConsumer.scala:270)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaBatchPartitionReader.next(KafkaBatchPartitionReader.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:133)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n",
      "  at scala.Option.foreach(Option.scala:407)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:815)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1423)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.RDD.take(RDD.scala:1396)\n",
      "  at org.apache.spark.rdd.RDD.$anonfun$isEmpty$1(RDD.scala:1531)\n",
      "  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "  at org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1531)\n",
      "  at org.apache.spark.api.java.JavaRDDLike.isEmpty(JavaRDDLike.scala:544)\n",
      "  at org.apache.spark.api.java.JavaRDDLike.isEmpty$(JavaRDDLike.scala:544)\n",
      "  at org.apache.spark.api.java.AbstractJavaRDDLike.isEmpty(JavaRDDLike.scala:45)\n",
      "  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:157)\n",
      "  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:126)\n",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:124)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:123)\n",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n",
      "  at MTASubwayTripUpdates$.myFunc$1(<console>:181)\n",
      "  at MTASubwayTripUpdates$.$anonfun$start$1(<console>:188)\n",
      "  at MTASubwayTripUpdates$.$anonfun$start$1$adapted(<console>:188)\n",
      "  at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:36)\n",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:572)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:570)\n",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:570)\n",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)\n",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "  at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "  ... 1 more\n",
      "Caused by: java.lang.NoSuchMethodError: org.apache.spark.kafka010.KafkaTokenUtil$.needTokenUpdate(Lorg/apache/spark/SparkConf;Ljava/util/Map;Lscala/Option;)Z\n",
      "  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getOrRetrieveConsumer(KafkaDataConsumer.scala:532)\n",
      "  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$get$1(KafkaDataConsumer.scala:274)\n",
      "  at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:587)\n",
      "  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.get(KafkaDataConsumer.scala:270)\n",
      "  at org.apache.spark.sql.kafka010.KafkaBatchPartitionReader.next(KafkaBatchPartitionReader.scala:63)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\n",
      "  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)\n",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:133)\n",
      "  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "  at java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MTASubwayTripUpdates.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
